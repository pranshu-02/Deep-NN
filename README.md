# Deep-NN

Implemented a program for training a deep neural network from scratch.  

Implements a [LINEAR->RELU]*(L-1)-> LINEAR-> SIGMOID Model, i.e uses RELU Activation Function for starting L-1 layers and Sigmoid Activation Function for the last layer.  

Uses Adam Optimizer and L2 Regularization to improve performance.

Test Results:  
Got 98.1% Accuracy On The MNIST Handwritten Digit Dataset
